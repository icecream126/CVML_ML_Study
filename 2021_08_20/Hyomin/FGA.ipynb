{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FGA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl6_1pyHzXds",
        "outputId": "4f8347ac-f3af-48d1-cd49-f32879226bbb"
      },
      "source": [
        "!pip install deeprobust"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeprobust\n",
            "  Downloading deeprobust-0.2.2-py3-none-any.whl (184 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 102 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 112 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 122 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 133 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 143 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 153 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 163 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 174 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy>=1.17.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.19.5)\n",
            "Collecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: scikit-image>=0.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.16.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.9.0+cu102)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.51.2)\n",
            "Collecting gensim<4.0,>=3.8\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 100 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.4.1)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (7.1.2)\n",
            "Collecting tensorboardX>=2.0\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (2.6.2)\n",
            "Requirement already satisfied: tqdm>=3.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (4.62.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.22.2.post1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8->deeprobust) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8->deeprobust) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (2.4.7)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (57.4.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->deeprobust) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.0->deeprobust) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->deeprobust) (3.7.4.3)\n",
            "Installing collected packages: texttable, tensorboardX, gensim, deeprobust\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed deeprobust-0.2.2 gensim-3.8.3 tensorboardX-2.4 texttable-1.6.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r26sdF_Mylck",
        "outputId": "23475282-d20e-4f19-80d9-3ba1630846a3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import f1_score\n",
        "from deeprobust.graph.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No module named 'torch_geometric'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/deeprobust/graph/data/__init__.py:11: UserWarning: Please install pytorch geometric if you would like to use the datasets from pytorch geometric. See details in https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n",
            "  \"geometric. See details in https://pytorch-geom\" +\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye68IqNd2Dlz"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKWMi9INzeIj"
      },
      "source": [
        "def classification_margin(output,true_label):\n",
        "  '''\n",
        "  Output 에 대한 classification margin 값을 구합니다.\n",
        "  probs_true_label - probs_best_second_class\n",
        "\n",
        "  Parameters :\n",
        "  1. output \n",
        "    자료형 : torch.Tensor\n",
        "    설명 : output vector (1차원)\n",
        "  \n",
        "  2. true_label\n",
        "    자료형 : int\n",
        "    설명 : 해당 노드의 true label \n",
        "  \n",
        "  Return값 :\n",
        "  1. list\n",
        "    설명 : 해당 노드의 classification margin\n",
        "\n",
        "  '''\n",
        "  probs = torch.exp(output)\n",
        "  probs_true_label = probs[true_label].clone()\n",
        "  probs[true_label] = 0 # true label의 probs값은 0으로 만들어놓고 argmax를 통해 두번째로 probs가 높은 값을 찾는다.\n",
        "  probs_best_second_class = probs[probs.argmax()]\n",
        "  return (probs_true_label - probs_best_second_class).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsJZuhFC2G6Z"
      },
      "source": [
        "def normalize_adj(mx) : \n",
        "  '''\n",
        "  희소 인접 행렬(Sparse adjacency matrix)을 Normalize 합니다.\n",
        "  A' = (D+I)^(-1/2) * (A+I)*(D+I)^(-1/2)\n",
        "\n",
        "  Parameters : \n",
        "  1. mx\n",
        "    자료형 : scipy.sparse.csr_matrix\n",
        "    설명 : normalize 할 matrix\n",
        "  \n",
        "  Returns : \n",
        "  자료형 : scipy.sparse.lil_matrix\n",
        "  설명 : normalize 된 matrix\n",
        "  '''\n",
        "\n",
        "  # TODO : coo format을 사용하는 것이 더 나을지도?\n",
        "  if type(mx) is not sp.lil.lil_matrix:\n",
        "    mx = mx.tolil()\n",
        "  if mx[0,0] == 0:\n",
        "    # 만약 I 행렬이 더해져 있지 않은 상태이면 I 행렬을 더해줌(self-loop)\n",
        "    mx = mx+sp.eye(mx.shape[0])\n",
        "  rowsum = np.array(mx.sum(1))\n",
        "  r_inv = np.power(rowsum, -1/2).flatten()\n",
        "  r_inv[np.isinf(r_inv)] = 0.\n",
        "  r_mat_inv = sp.diags(r_inv)\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  mx = mx.dot(r_mat_inv)\n",
        "  return mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC1ZudsT3YNx"
      },
      "source": [
        "def to_scipy(tensor):\n",
        "  ''' Dense / Sparse tensor를 scipy matrix로 변환합니다'''\n",
        "  if is_sparse_tensor(tensor):\n",
        "    values = tensor._values()\n",
        "    indices = tensor._indices()\n",
        "    return sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()), shape = tensor.shape)\n",
        "  else:\n",
        "    indices = tensor.nonzero().t()\n",
        "    values = tensor[indices[0], indices[1]]\n",
        "    return sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()), shape = tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6_1KcJ26OKp"
      },
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "  ''' scipy sparse matrix를 torch sparse tensor로 변환합니다'''\n",
        "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "  sparserow = torch.LongTensor(sparse_mx.row).unsqueeze(1)\n",
        "  sparsecol = torch.LongTensor(sparse_mx.col).unsqueeze(1)\n",
        "  sparseconcat = torch.cat((sparserow, sparsecol),1)\n",
        "  sparsedata = torch.FloatTensor(sparse_mx.data)\n",
        "  return torch.sparse.FloatTensor(sparseconcat.t(), sparsedata, torch.Size(sparse_mx.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo-I9eCQ6r-I"
      },
      "source": [
        "def to_tensor(adj, features, labels = None, device = 'cpu'):\n",
        "  '''\n",
        "  인접행렬(adj), Feature행렬(features), 라벨(labels)들을 array 혹은 sparse matrix에서 \n",
        "  torch Tensor 로 바꿔줍니다.\n",
        "\n",
        "  Parameters : \n",
        "  1. adj\n",
        "    자료형 : scipy.sparse.csr_matrix\n",
        "    설명 : 인접행렬\n",
        "  2. features\n",
        "    자료형 : scipy .sparse.csr_matrix\n",
        "    설명 : 노드 features\n",
        "  3. labels\n",
        "    자료형 : numpy.array\n",
        "    설명 : 노드 라벨\n",
        "  4. device\n",
        "    자료형 : str\n",
        "    설명 : cpu or cuda\n",
        "  '''\n",
        "  # Adj matrix 변환\n",
        "  if sp.issparse(adj):\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "  else:\n",
        "    adj = torch.FloatTensor(adj)\n",
        "  \n",
        "  # Feature matrix 변환\n",
        "  if sp.issparse(features):\n",
        "    features = sparse_mx_to_torch_sparse_tensor(features)\n",
        "  else:\n",
        "    features = torch.FloatTensor(features)\n",
        "  \n",
        "  # label 변환\n",
        "  if labels is None:\n",
        "    return adj.to(device), features.to(device)\n",
        "  else:\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return adj.to(device), features.to(device), labels.to(device)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exen1l4Q74Bj"
      },
      "source": [
        "def normalize_adj_tensor(adj, sparse=False):\n",
        "  ''' Adjacency tensor matrix를 normalize 합니다'''\n",
        "  device = torch.device('cuda' if adj.is_cuda else 'cpu')\n",
        "  if sparse:\n",
        "    adj = to_scipy(adj)\n",
        "    mx = normalize_adj(adj)\n",
        "    return sparse_mx_to_torch_sparse_tensor(mx).to(device)\n",
        "  else:\n",
        "    mx = adj+torch.eye(adj.shape[0]).to(device)\n",
        "    rowsum = mx.sum(1)\n",
        "    r_inv = rowsum.pow(-1/2).flatten()\n",
        "    r_inv[torch.isinf(r_inv)]=0.\n",
        "    r_mat_inv = torch.diag(r_inv)\n",
        "    mx = r_mat_inv @ mx\n",
        "    mx = mx @ r_mat_inv\n",
        "  return mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vFKsW1n8nCU"
      },
      "source": [
        "def get_train_val_test(nnodes, val_size=0.1, test_size=0.8, stratify = None, seed=None):\n",
        "  '''\n",
        "  Nettack / Mettack의 설정값을 따릅니다.\n",
        "  노드 중 10%는 training, 10%는 validation, 나머지 80%는 test용으로 사용합니다.\n",
        "\n",
        "  Parameters:\n",
        "  1. nnodes\n",
        "    자료형 : int\n",
        "    설명 : 총 노드 개수\n",
        "  2. val_size\n",
        "    자료형 : float\n",
        "    설명 : validation set의 size\n",
        "  3. test_size \n",
        "    자료형 : float\n",
        "    설명 : test set의 size\n",
        "  4. stratify\n",
        "    설명 : data가 stratified fashion으로 나뉘어지게 될 것입니다. \n",
        "          따라서 stratify는 label이 되어야 합니다.\n",
        "  5. seed \n",
        "    자료형 : int or None\n",
        "    설명 : random seed\n",
        "\n",
        "  Returns :\n",
        "  1. idx_train\n",
        "    설명 : training node의 인덱스\n",
        "  2. idx_val\n",
        "    설명 : validation node의 인덱스\n",
        "  3. idx_test\n",
        "    설명 : test set의 인덱스\n",
        "  '''\n",
        "\n",
        "  assert stratify is not None, 'stratify cannot be None!'\n",
        "\n",
        "  if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "  idx = np.arange(nnodes)\n",
        "  train_size = 1 - val_size - test_size\n",
        "  idx_train_and_val, idx_test = train_test_split(idx,\n",
        "                                                   random_state=None,\n",
        "                                                   train_size=train_size + val_size,\n",
        "                                                   test_size=test_size,\n",
        "                                                   stratify=stratify)\n",
        "\n",
        "  if stratify is not None:\n",
        "      stratify = stratify[idx_train_and_val]\n",
        "\n",
        "  idx_train, idx_val = train_test_split(idx_train_and_val,\n",
        "                                          random_state=None,\n",
        "                                          train_size=(train_size / (train_size + val_size)),\n",
        "                                          test_size=(val_size / (train_size + val_size)),\n",
        "                                          stratify=stratify)\n",
        "\n",
        "  return idx_train, idx_val, idx_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoreOHCR96e9"
      },
      "source": [
        "def get_train_val_test_gcn(labels, seed=None):\n",
        "  '''\n",
        "  GCN setting을 따릅니다. 각 class별로 20개의 인스턴스를 training data로 랜덤하게\n",
        "  샘플링하고, 500개의 인스턴스를 validation data로, 1000개의 인스턴스를 test data로\n",
        "  샘플링합니다. Random seed가 바뀌면 split도 바뀌게 됩니다.\n",
        "\n",
        "  Parameters : \n",
        "  1. labels\n",
        "    자료형 : numpy.array\n",
        "    설명 : 노드 라벨\n",
        "  2. seed\n",
        "    자료형 : int 혹은 None\n",
        "    설명 : random seed\n",
        "\n",
        "  Returns : \n",
        "  1. idx_train\n",
        "    설명 : training node 인덱스\n",
        "  2. idx_val\n",
        "    설명 : validation node 인덱스\n",
        "  3. idx_test\n",
        "    설명 : test node 인덱스\n",
        "  '''\n",
        "\n",
        "  if seed is not None:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "  idx = np.arange(len(labels))\n",
        "  nclass = labels.max() + 1\n",
        "  idx_train = []\n",
        "  idx_unlabeled = []\n",
        "  for i in range(nclass):\n",
        "      labels_i = idx[labels==i]\n",
        "      labels_i = np.random.permutation(labels_i)\n",
        "      idx_train = np.hstack((idx_train, labels_i[: 20])).astype(np.int)\n",
        "      idx_unlabeled = np.hstack((idx_unlabeled, labels_i[20: ])).astype(np.int)\n",
        "\n",
        "  idx_unlabeled = np.random.permutation(idx_unlabeled)\n",
        "  idx_val = idx_unlabeled[: 500]\n",
        "  idx_test = idx_unlabeled[500: 1500]\n",
        "  return idx_train, idx_val, idx_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-UMPT9_HU8"
      },
      "source": [
        "def is_sparse_tensor(tensor):\n",
        "  '''\n",
        "  tensor가 sparse tensor 인지 확인합니다.\n",
        "\n",
        "  Parameters : \n",
        "  1. tensor\n",
        "    자료형 : torch.Tensor\n",
        "    설명 : 주어진 tensor\n",
        "\n",
        "  Returns : \n",
        "  1. bool : tensor가 sparse tensor인지 아닌지에 대한 결과\n",
        "\n",
        "  '''\n",
        "  if tensor.layout == torch.sparse_coo:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qmfj_KZ_5aX"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "  '''\n",
        "  output과 label을 비교했을 때의 accuracy값을 도출합니다.\n",
        "\n",
        "  Parameters : \n",
        "  1. output \n",
        "    자료형 : torch.Tensor\n",
        "    설명 : 모델에서 나온 output 결과\n",
        "  2. labels\n",
        "    자료형 : torch.Tensor 혹은 numpy.array\n",
        "    설명 : 노드 라벨\n",
        "\n",
        "  Returns : \n",
        "  1. float : accuray 정보\n",
        "\n",
        "  '''\n",
        "  if not hasattr(labels, '__len__'):\n",
        "      labels = [labels]\n",
        "  if type(labels) is not torch.Tensor:\n",
        "      labels = torch.LongTensor(labels)\n",
        "  preds = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84L2eDAWK-h_"
      },
      "source": [
        "# BaseAttack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgZvSDw6AYFF"
      },
      "source": [
        "class BaseAttack(Module):\n",
        "    \"\"\"Abstract base class for target attack classes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model :\n",
        "        model to attack\n",
        "    nnodes : int\n",
        "        number of nodes in the input graph\n",
        "    attack_structure : bool\n",
        "        whether to attack graph structure\n",
        "    attack_features : bool\n",
        "        whether to attack node features\n",
        "    device: str\n",
        "        'cpu' or 'cuda'\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, nnodes, attack_structure=True, attack_features=False, device='cpu'):\n",
        "        super(BaseAttack, self).__init__()\n",
        "\n",
        "        self.surrogate = model\n",
        "        self.nnodes = nnodes\n",
        "        self.attack_structure = attack_structure\n",
        "        self.attack_features = attack_features\n",
        "        self.device = device\n",
        "\n",
        "        if model is not None:\n",
        "            self.nclass = model.nclass\n",
        "            self.nfeat = model.nfeat\n",
        "            self.hidden_sizes = model.hidden_sizes\n",
        "\n",
        "        self.modified_adj = None\n",
        "        self.modified_features = None\n",
        "\n",
        "    def attack(self, ori_adj, n_perturbations, **kwargs):\n",
        "        \"\"\"\n",
        "        입력된 Graph에 대해 perturbation을 가합니다.\n",
        "\n",
        "        Parameters:\n",
        "        1. ori_adj\n",
        "          자료형 : scipy.sparse.csr_matrix\n",
        "          설명 : 원래(unperturbed) 인접행렬\n",
        "        2. n_perturbations\n",
        "          자료형 : int\n",
        "          설명 : Input graph에 대한 perturbation 개수. \n",
        "                 Perturbation은 간선 제거/추가 혹은 feature 제거/추가가 될 수 있다.\n",
        "        \n",
        "        Returns : \n",
        "        없음\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    def check_adj(self, adj):\n",
        "      '''\n",
        "      인접행렬이 대칭이고 가중치가 없는지 확인합니다.\n",
        "      '''\n",
        "\n",
        "      if type(adj) is torch.Tensor:\n",
        "          adj = adj.cpu().numpy()\n",
        "      assert np.abs(adj - adj.T).sum() == 0, \"Input graph is not symmetric\"\n",
        "      if sp.issparse(adj):\n",
        "          assert adj.tocsr().max() == 1, \"Max value should be 1!\"\n",
        "          assert adj.tocsr().min() == 0, \"Min value should be 0!\"\n",
        "      else:\n",
        "          assert adj.max() == 1, \"Max value should be 1!\"\n",
        "          assert adj.min() == 0, \"Min value should be 0!\"\n",
        "\n",
        "\n",
        "    def save_adj(self, root=r'/tmp/', name='mod_adj'):\n",
        "        \"\"\"\n",
        "        공격이 들어간 인접행렬을 저장합니다.\n",
        "\n",
        "        Parameters:\n",
        "        1. root : \n",
        "          설명 : 변수가 저장될 root directory\n",
        "        2. name : \n",
        "          자료형 : str\n",
        "          설명 : 저장할 파일 이름\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        없음\n",
        "\n",
        "        \"\"\"\n",
        "        assert self.modified_adj is not None, \\\n",
        "                'modified_adj is None! Please perturb the graph first.'\n",
        "        name = name + '.npz'\n",
        "        modified_adj = self.modified_adj\n",
        "\n",
        "        if type(modified_adj) is torch.Tensor:\n",
        "            sparse_adj = utils.to_scipy(modified_adj)\n",
        "            sp.save_npz(osp.join(root, name), sparse_adj)\n",
        "        else:\n",
        "            sp.save_npz(osp.join(root, name), modified_adj)\n",
        "\n",
        "\n",
        "    def save_features(self, root=r'/tmp/', name='mod_features'):\n",
        "        \"\"\"\n",
        "        공격이 들어간 node feature matrix를 저장합니다.\n",
        "\n",
        "         Parameters:\n",
        "        1. root : \n",
        "          설명 : 변수가 저장될 root directory\n",
        "        2. name : \n",
        "          자료형 : str\n",
        "          설명 : 저장할 파일 이름\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        없음\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        assert self.modified_features is not None, \\\n",
        "                'modified_features is None! Please perturb the graph first.'\n",
        "        name = name + '.npz'\n",
        "        modified_features = self.modified_features\n",
        "\n",
        "        if type(modified_features) is torch.Tensor:\n",
        "            sparse_features = utils.to_scipy(modified_features)\n",
        "            sp.save_npz(osp.join(root, name), sparse_features)\n",
        "        else:\n",
        "            sp.save_npz(osp.join(root, name), modified_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bO5635yMrZv"
      },
      "source": [
        "# FGA (Fast Gradient Attack)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlaTzbdMbs4"
      },
      "source": [
        "'''     FGA: Fast Gradient Attack on Network Embedding (https://arxiv.org/pdf/1809.02797.pdf) '''\n",
        "class FGA(BaseAttack):\n",
        "  \"\"\"FGA/FGSM.\n",
        "\n",
        "  Parameters:\n",
        "  1. model\n",
        "    설명 : 공격할 모델\n",
        "\n",
        "  2. nnodes\n",
        "    자료형 ; int\n",
        "    설명 : input graph에 있는 노드 개수\n",
        "  \n",
        "  3. feature_shape\n",
        "    자료형 : tuple\n",
        "    설명 : input node feature의 shape\n",
        "\n",
        "  4. attack_structure\n",
        "    자료형 : bool\n",
        "    설명 : graph structure를 공격할지 말지 정하는 변수\n",
        "\n",
        "  5. attack_features\n",
        "    자료형 : bool\n",
        "    설명 : node features를 공격할지 말지 정하는 변수\n",
        "\n",
        "  6. device\n",
        "    자료형 : str\n",
        "    설명 ; 'cpu' or 'cuda'\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nnodes, feature_shape=None, attack_structure=True, attack_features=False, device='cpu'):\n",
        "\n",
        "        super(FGA, self).__init__(model, nnodes, attack_structure=attack_structure, attack_features=attack_features, device=device)\n",
        "\n",
        "\n",
        "        assert not self.attack_features, \"not support attacking features\"\n",
        "\n",
        "        if self.attack_features:\n",
        "            self.feature_changes = Parameter(torch.FloatTensor(feature_shape))\n",
        "            self.feature_changes.data.fill_(0)\n",
        "  def attack(self, ori_features, ori_adj, labels, idx_train, target_node, n_perturbations, verbose=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Input graph에 대해 perturbation을 가합니다.\n",
        "\n",
        "        Parameters:\n",
        "        1. ori_features\n",
        "          자료형 : scipy.sparse.csr_matrix\n",
        "          설명 : 원본(unperturbed) adjacency matrix\n",
        "\n",
        "        2. ori_adj\n",
        "          자료형 : scipy.sparse.csr_matrix\n",
        "          설명 : 원본(unperturbed) node feature matrix\n",
        "\n",
        "        3. labels \n",
        "          설명 : 노드 라벨\n",
        "\n",
        "        4. idx_train\n",
        "          설명 : training node 인덱스\n",
        "        \n",
        "        5. target_node \n",
        "          자료형 : int\n",
        "          설명 : 공격될 타겟 노드의 인덱스\n",
        "        \n",
        "        6. n_perturbations\n",
        "          자료형 : int\n",
        "          설명 : Input graph에 들어갈 perturbation 수. Perturbation은\n",
        "                 간선 추가/삭제 혹은 feature 추가/삭제가 될 수 있다.\n",
        "        \"\"\"\n",
        "        # todense() : 희소행렬을 압축희소행렬로 만들어서 메모리낭비와 연산 시간을 줄임\n",
        "        modified_adj = ori_adj.todense()\n",
        "        modified_features = ori_features.todense()\n",
        "        modified_adj, modified_features, labels = to_tensor(modified_adj, modified_features, labels, device=self.device) # tensor 형태로 반환\n",
        "\n",
        "        self.surrogate.eval()\n",
        "        if verbose == True:\n",
        "            print('number of pertubations: %s' % n_perturbations)\n",
        "\n",
        "        pseudo_labels = self.surrogate.predict().detach().argmax(1)\n",
        "        pseudo_labels[idx_train] = labels[idx_train]\n",
        "\n",
        "        modified_adj.requires_grad = True\n",
        "        for i in range(n_perturbations):\n",
        "            adj_norm = normalize_adj_tensor(modified_adj)\n",
        "\n",
        "            if self.attack_structure:\n",
        "                output = self.surrogate(modified_features, adj_norm)\n",
        "                loss = F.nll_loss(output[[target_node]], pseudo_labels[[target_node]])\n",
        "                grad = torch.autograd.grad(loss, modified_adj)[0]\n",
        "                # bidirection\n",
        "                grad = (grad[target_node] + grad[:, target_node]) * (-2*modified_adj[target_node] + 1)\n",
        "                grad[target_node] = -10\n",
        "                grad_argmax = torch.argmax(grad)\n",
        "\n",
        "            value = -2*modified_adj[target_node][grad_argmax] + 1 # target node의 row에 있는 값중 grad가 가장 높은 node값을 이용해 value를 구함\n",
        "            modified_adj.data[target_node][grad_argmax] += value # 앞서 구한 value값을 양방향으로 더해줌 (undirected : g_ij, g_ji)\n",
        "            modified_adj.data[grad_argmax][target_node] += value # 양방향으로 더해줌.\n",
        "\n",
        "            if self.attack_features:\n",
        "                pass\n",
        "\n",
        "        modified_adj = modified_adj.detach().cpu().numpy()\n",
        "        modified_adj = sp.csr_matrix(modified_adj)\n",
        "        self.check_adj(modified_adj)\n",
        "        self.modified_adj = modified_adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtu6spICPgPs"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsinLO55Pf1G"
      },
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    간단한 GCN layer, https://github.com/tkipf/pygcn 참고\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, with_bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if with_bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        \"\"\" \n",
        "        Forward 함수\n",
        "        \"\"\"\n",
        "        if input.data.is_sparse:\n",
        "            support = torch.spmm(input, self.weight)\n",
        "        else:\n",
        "            support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \"\"\" \n",
        "    2 계층 GCN\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    nfeat : int\n",
        "        size of input feature dimension\n",
        "    nhid : int\n",
        "        number of hidden units\n",
        "    nclass : int\n",
        "        size of output dimension\n",
        "    dropout : float\n",
        "        dropout rate for GCN\n",
        "    lr : float\n",
        "        learning rate for GCN\n",
        "    weight_decay : float\n",
        "        weight decay coefficient (l2 normalization) for GCN.\n",
        "        When `with_relu` is True, `weight_decay` will be set to 0.\n",
        "    with_relu : bool\n",
        "        whether to use relu activation function. If False, GCN will be linearized.\n",
        "    with_bias: bool\n",
        "        whether to include bias term in GCN weights.\n",
        "    device: str\n",
        "        'cpu' or 'cuda'.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, lr=0.01, weight_decay=5e-4,\n",
        "            with_relu=True, with_bias=True, device=None):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        assert device is not None, \"Please specify 'device'!\"\n",
        "        self.device = device\n",
        "        self.nfeat = nfeat\n",
        "        self.hidden_sizes = [nhid]\n",
        "        self.nclass = nclass\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid, with_bias=with_bias)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass, with_bias=with_bias)\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        if not with_relu:\n",
        "            self.weight_decay = 0\n",
        "        else:\n",
        "            self.weight_decay = weight_decay\n",
        "        self.with_relu = with_relu\n",
        "        self.with_bias = with_bias\n",
        "        self.output = None\n",
        "        self.best_model = None\n",
        "        self.best_output = None\n",
        "        self.adj_norm = None\n",
        "        self.features = None\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        if self.with_relu:\n",
        "            x = F.relu(self.gc1(x, adj))\n",
        "        else:\n",
        "            x = self.gc1(x, adj)\n",
        "\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        GCN 파라미터 초기화\n",
        "        \"\"\"\n",
        "        self.gc1.reset_parameters()\n",
        "        self.gc2.reset_parameters()\n",
        "\n",
        "\n",
        "    def fit(self, features, adj, labels, idx_train, idx_val=None, train_iters=200, initialize=True, verbose=False, normalize=True, patience=500, **kwargs):\n",
        "        \"\"\"\n",
        "        GCN model 훈련, idx_val이 None이 아니라면 validation loss에 따른 best model을 선택한다.\n",
        "\n",
        "        Parameters:\n",
        "        1. features\n",
        "          설명 : 노드 features\n",
        "        \n",
        "        2. adj\n",
        "          자료형 : torch.tensor 혹은 scipy matrix\n",
        "\n",
        "        3. labels\n",
        "          설명 : 노드 라벨\n",
        "        \n",
        "        4. idx_train\n",
        "          설명 : training node 인덱스\n",
        "\n",
        "        5. idx_val\n",
        "          설명 : validation node 인덱스. 주어지지 않은 경우 (None 인 경우), GCN 훈련 과정에서 early stopping을 적용하지 않을 예정\n",
        "        \n",
        "        6. train_iters\n",
        "          자료형 : int\n",
        "          설명 : training epoch 수\n",
        "\n",
        "        7. initialize\n",
        "          자료형 : bool\n",
        "          설명 : training 이전에 파라미터를 초기화할 것인지 말것인지를 결정하는 변수\n",
        "\n",
        "        8. verbose\n",
        "          자료형 : bool\n",
        "          설명 : 상세 모드 할 것인지 말 것인지를 결정\n",
        "\n",
        "        9. normalize\n",
        "          자료형 : bool\n",
        "          설명 : input 인접 행렬을 normalize 할 것인지 말 것인지\n",
        "        \n",
        "        10. patience\n",
        "          자료형 : int\n",
        "          설명 : early stopping을 위한 patience, idx_val이 주어진 경우에만 유효함.\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = self.gc1.weight.device\n",
        "        if initialize:\n",
        "            self.initialize()\n",
        "\n",
        "        if type(adj) is not torch.Tensor:\n",
        "            features, adj, labels = to_tensor(features, adj, labels, device=self.device) # utils 없앰 **\n",
        "\n",
        "        else:\n",
        "            features = features.to(self.device)\n",
        "            adj = adj.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "        if normalize:\n",
        "            if is_sparse_tensor(adj):\n",
        "                adj_norm = normalize_adj_tensor(adj, sparse=True)\n",
        "            else:\n",
        "                adj_norm = normalize_adj_tensor(adj)\n",
        "        else:\n",
        "            adj_norm = adj\n",
        "\n",
        "        self.adj_norm = adj_norm\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "        if idx_val is None:\n",
        "            self._train_without_val(labels, idx_train, train_iters, verbose)\n",
        "        else:\n",
        "            if patience < train_iters:\n",
        "                self._train_with_early_stopping(labels, idx_train, idx_val, train_iters, patience, verbose)\n",
        "            else:\n",
        "                self._train_with_val(labels, idx_train, idx_val, train_iters, verbose)\n",
        "\n",
        "\n",
        "    def _train_without_val(self, labels, idx_train, train_iters, verbose):\n",
        "      '''idx_val이 주어지지 않은 경우의 training 과정'''\n",
        "      self.train()\n",
        "      optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "      for i in range(train_iters):\n",
        "          optimizer.zero_grad()\n",
        "          output = self.forward(self.features, self.adj_norm)\n",
        "          loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "          loss_train.backward()\n",
        "          optimizer.step()\n",
        "          if verbose and i % 10 == 0:\n",
        "              print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
        "\n",
        "      self.eval()\n",
        "      output = self.forward(self.features, self.adj_norm)\n",
        "      self.output = output\n",
        "\n",
        "    def _train_with_val(self, labels, idx_train, idx_val, train_iters, verbose):\n",
        "      '''idx_val이 주어진 경우의 train 과정'''\n",
        "      if verbose:\n",
        "          print('=== training gcn model ===')\n",
        "      optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "      best_loss_val = 100\n",
        "      best_acc_val = 0\n",
        "\n",
        "      for i in range(train_iters):\n",
        "          self.train()\n",
        "          optimizer.zero_grad()\n",
        "          output = self.forward(self.features, self.adj_norm)\n",
        "          loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "          loss_train.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if verbose and i % 10 == 0:\n",
        "              print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
        "\n",
        "          self.eval()\n",
        "          output = self.forward(self.features, self.adj_norm)\n",
        "          loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "          # acc_val = utils.accuracy(output[idx_val], labels[idx_val])\n",
        "          acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "          if best_loss_val > loss_val:\n",
        "              best_loss_val = loss_val\n",
        "              self.output = output\n",
        "              weights = deepcopy(self.state_dict())\n",
        "\n",
        "          if acc_val > best_acc_val:\n",
        "              best_acc_val = acc_val\n",
        "              self.output = output\n",
        "              weights = deepcopy(self.state_dict())\n",
        "\n",
        "      if verbose:\n",
        "          print('=== picking the best model according to the performance on validation ===')\n",
        "      self.load_state_dict(weights)\n",
        "\n",
        "    def _train_with_early_stopping(self, labels, idx_train, idx_val, train_iters, patience, verbose):\n",
        "      '''early_stopping을 적용한 training 과정. idx_val이 있는 경우에만 유효하다.'''\n",
        "      if verbose:\n",
        "          print('=== training gcn model ===')\n",
        "      optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "      early_stopping = patience\n",
        "      best_loss_val = 100\n",
        "\n",
        "      for i in range(train_iters):\n",
        "          self.train()\n",
        "          optimizer.zero_grad()\n",
        "          output = self.forward(self.features, self.adj_norm)\n",
        "          loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "          loss_train.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if verbose and i % 10 == 0:\n",
        "              print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
        "\n",
        "          self.eval()\n",
        "          output = self.forward(self.features, self.adj_norm)\n",
        "\n",
        "            # def eval_class(output, labels):\n",
        "            #     preds = output.max(1)[1].type_as(labels)\n",
        "            #     return f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='micro') + \\\n",
        "            #         f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
        "\n",
        "            # perf_sum = eval_class(output[idx_val], labels[idx_val])\n",
        "          loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "\n",
        "          if best_loss_val > loss_val:\n",
        "              best_loss_val = loss_val\n",
        "              self.output = output\n",
        "              weights = deepcopy(self.state_dict())\n",
        "              patience = early_stopping\n",
        "          else:\n",
        "              patience -= 1\n",
        "          if i > early_stopping and patience <= 0:\n",
        "              break\n",
        "\n",
        "      if verbose:\n",
        "           print('=== early stopping at {0}, loss_val = {1} ==='.format(i, best_loss_val) )\n",
        "      self.load_state_dict(weights)\n",
        "\n",
        "    def test(self, idx_test):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        test set에 대하여 GCN의 성능을 측정한다.\n",
        "\n",
        "        Parameters:\n",
        "        1. idx_test\n",
        "\n",
        "        설명 : test node 인덱스\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        output = self.predict()\n",
        "        # output = self.output\n",
        "        loss_test = F.nll_loss(output[idx_test], self.labels[idx_test])\n",
        "        # acc_test = utils.accuracy(output[idx_test], self.labels[idx_test])\n",
        "        acc_test = accuracy(output[idx_test], self.labels[idx_test])\n",
        "        print(\"Test set results:\",\n",
        "              \"loss= {:.4f}\".format(loss_test.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "        return acc_test.item()\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, features=None, adj=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Default로는 input은 normalize 되지 않은 인접행렬이다.\n",
        "\n",
        "        Parameters:\n",
        "        1. features : \n",
        "          설명 : node features. 만약 'features'와 'adj'가 주어지지 않은 경우 이 함수는 이전에 training 할때 저장된 'features'와 'adj'를 사용하여 prediction을 할 것임\n",
        "\n",
        "        2. adj\n",
        "          설명 : 인접 행렬. 만약 'features'와 'adj'가 주어지지 않은 경우 이 함수는 이전에 training 할때 저장된 'features'와 'adj'를 사용하여 prediction을 할 것임\n",
        "\n",
        "        \n",
        "        Returns :\n",
        "\n",
        "        1. torch.FloatTensor\n",
        "           설명 :  output (log probabilities) of GCN\n",
        "        \"\"\"\n",
        "\n",
        "        self.eval()\n",
        "        if features is None and adj is None:\n",
        "            return self.forward(self.features, self.adj_norm)\n",
        "        else:\n",
        "            if type(adj) is not torch.Tensor:\n",
        "                # features, adj = utils.to_tensor(features, adj, device=self.device)\n",
        "                features, adj = to_tensor(features, adj, device=self.device)\n",
        "\n",
        "            self.features = features\n",
        "            # if utils.is_sparse_tensor(adj):\n",
        "            if is_sparse_tensor(adj):\n",
        "                # self.adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
        "                self.adj_norm = normalize_adj_tensor(adj, sparse=True)\n",
        "            else:\n",
        "                # self.adj_norm = utils.normalize_adj_tensor(adj)\n",
        "                self.adj_norm = normalize_adj_tensor(adj)\n",
        "            return self.forward(self.features, self.adj_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw6N5Ik_PZlR"
      },
      "source": [
        "# Attack 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHkYqrFdPAJw",
        "outputId": "760a51e4-eb17-4d6f-a449-58e89597f77b"
      },
      "source": [
        "data_name = 'cora' # args.dataset을 data_name으로 지정해줄 예정 **\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "np.random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "torch.cuda.manual_seed(777)\n",
        "\n",
        "data = Dataset(root='/tmp/',name=data_name) # data_name으로 바꿔줌 **\n",
        "adj, features, labels = data.adj, data.features, data.labels # data(cora)의 adj, features, labels 정보를 받아옴\n",
        "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test # data의 index 정보를 받아옴\n",
        "\n",
        "idx_unlabeled = np.union1d(idx_val, idx_test) # unlabeled 된 index 정보를 받아옴\n",
        "\n",
        "# Setup Surrogate model\n",
        "surrogate = GCN(nfeat=features.shape[1], nclass=labels.max().item()+1,\n",
        "                nhid=16, device=device)\n",
        "\n",
        "surrogate = surrogate.to(device)\n",
        "surrogate.fit(features, adj, labels, idx_train, idx_val)\n",
        "\n",
        "# Setup Attack Model\n",
        "target_node = 0\n",
        "model = FGA(surrogate, nnodes=adj.shape[0], device=device)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Downloading from https://raw.githubusercontent.com/danielzuegner/gnn-meta-attack/master/data/cora.npz to /tmp/cora.npz\n",
            "Done!\n",
            "Selecting 1 largest connected components\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoAYXbWOPbXQ"
      },
      "source": [
        "def main():\n",
        "    u = 0 # node to attack\n",
        "    assert u in idx_unlabeled # target node는 train과정에 있으면 안됨 \n",
        "\n",
        "    degrees = adj.sum(0).A1 \n",
        "    n_perturbations = int(degrees[u]) # perturbation 을 얼마나 많이 할 것인지. Default : 노드의 degree 값\n",
        "\n",
        "    model.attack(features, adj, labels, idx_train, target_node, n_perturbations)\n",
        "\n",
        "    print('=== testing GCN on original(clean) graph ===')\n",
        "    test(adj, features, target_node)\n",
        "\n",
        "    print('=== testing GCN on perturbed graph ===')\n",
        "    test(model.modified_adj, features, target_node)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsbyjvu2TOYN"
      },
      "source": [
        "def test(adj, features, target_node):\n",
        "    ''' test on GCN '''\n",
        "    gcn = GCN(nfeat=features.shape[1],\n",
        "              nhid=16,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=0.5, device=device)\n",
        "\n",
        "    gcn = gcn.to(device)\n",
        "\n",
        "    gcn.fit(features, adj, labels, idx_train)\n",
        "\n",
        "    gcn.eval()\n",
        "    output = gcn.predict()\n",
        "    probs = torch.exp(output[[target_node]])[0]\n",
        "    print('probs: {}'.format(probs.detach().cpu().numpy()))\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "\n",
        "    print(\"Test set results:\",\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "    return acc_test.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnuJHXrxTQfg"
      },
      "source": [
        "def select_nodes(target_gcn=None):\n",
        "    '''\n",
        "    nettack 논문에 쓰인 방식대로 node를 선택한다.\n",
        "    1. 가장 높은 classification margin값을 가지는 10개의 node (clearly correctly classified)\n",
        "    2. 가장 낮은 classification margin 값을 가지는 10개의 node(여전히 올바르게 분류됨)\n",
        "    3. 20개의 노드를 추가로 랜덤하게\n",
        "    '''\n",
        "\n",
        "    if target_gcn is None:\n",
        "        target_gcn = GCN(nfeat=features.shape[1],\n",
        "                  nhid=16,\n",
        "                  nclass=labels.max().item() + 1,\n",
        "                  dropout=0.5, device=device)\n",
        "        target_gcn = target_gcn.to(device)\n",
        "        target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "    target_gcn.eval()\n",
        "    output = target_gcn.predict()\n",
        "\n",
        "    margin_dict = {}\n",
        "    for idx in idx_test:\n",
        "        margin = classification_margin(output[idx], labels[idx])\n",
        "        if margin < 0: # margin이 0 이하인 node는 고려하지 않음(올바르게 분류된 노드만 고려)\n",
        "            continue\n",
        "        margin_dict[idx] = margin\n",
        "    sorted_margins = sorted(margin_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "    high = [x for x, y in sorted_margins[: 10]]\n",
        "    low = [x for x, y in sorted_margins[-10: ]]\n",
        "    other = [x for x, y in sorted_margins[10: -10]]\n",
        "    other = np.random.choice(other, 20, replace=False).tolist()\n",
        "\n",
        "    return high + low + other"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YI2t4hMTTFR"
      },
      "source": [
        "def multi_test_poison():\n",
        "    # poisoning attack 환경에서 40개의 node에 대해 test\n",
        "    cnt = 0\n",
        "    degrees = adj.sum(0).A1\n",
        "    node_list = select_nodes()\n",
        "    num = len(node_list)\n",
        "    print('=== [Poisoning] Attacking %s nodes respectively ===' % num)\n",
        "    for target_node in tqdm(node_list):\n",
        "        n_perturbations = int(degrees[target_node])\n",
        "        model = FGA(surrogate, nnodes=adj.shape[0], device=device)\n",
        "        model = model.to(device)\n",
        "        model.attack(features, adj, labels, idx_train, target_node, n_perturbations)\n",
        "        modified_adj = model.modified_adj\n",
        "        acc = single_test(modified_adj, features, target_node)\n",
        "        if acc == 0:\n",
        "            cnt += 1\n",
        "    print('misclassification rate : %s' % (cnt/num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umq7mwiFTU7I"
      },
      "source": [
        "def single_test(adj, features, target_node, gcn=None):\n",
        "    if gcn is None:\n",
        "        # poisoning attack을 GCN에서 test\n",
        "        gcn = GCN(nfeat=features.shape[1],\n",
        "                  nhid=16,\n",
        "                  nclass=labels.max().item() + 1,\n",
        "                  dropout=0.5, device=device)\n",
        "\n",
        "        gcn = gcn.to(device)\n",
        "\n",
        "        gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "        gcn.eval()\n",
        "        output = gcn.predict()\n",
        "    else:\n",
        "        # evasion attack을 GCN에서 test\n",
        "        output = gcn.predict(features, adj)\n",
        "    probs = torch.exp(output[[target_node]])\n",
        "\n",
        "    # acc_test = accuracy(output[[target_node]], labels[target_node])\n",
        "    acc_test = (output.argmax(1)[target_node] == labels[target_node])\n",
        "    return acc_test.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5d_lqIoTXAn"
      },
      "source": [
        "def multi_test_evasion():\n",
        "    # evasion attack을 40개의 node에서 test\n",
        "    # target_gcn = GCN(nfeat=features.shape[1],\n",
        "    #           nhid=16,\n",
        "    #           nclass=labels.max().item() + 1,\n",
        "    #           dropout=0.5, device=device)\n",
        "\n",
        "    # target_gcn = target_gcn.to(device)\n",
        "    # target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "\n",
        "    target_gcn = surrogate\n",
        "    cnt = 0\n",
        "    degrees = adj.sum(0).A1\n",
        "    node_list = select_nodes(target_gcn)\n",
        "    num = len(node_list)\n",
        "\n",
        "    print('=== [Evasion] Attacking %s nodes respectively ===' % num)\n",
        "    for target_node in tqdm(node_list):\n",
        "        n_perturbations = int(degrees[target_node])\n",
        "        model = FGA(surrogate, nnodes=adj.shape[0], device=device)\n",
        "        model = model.to(device)\n",
        "        model.attack(features, adj, labels, idx_train, target_node, n_perturbations)\n",
        "        modified_adj = model.modified_adj\n",
        "\n",
        "        acc = single_test(modified_adj, features, target_node, gcn=target_gcn)\n",
        "        if acc == 0:\n",
        "            cnt += 1 # accuracy 값이 0인걸 세어서 misclassification rate 계산에 이용\n",
        "    print('misclassification rate : %s' % (cnt/num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-3WqBWyTZBv",
        "outputId": "43a78699-7bdd-4eae-84f2-c5f14ef62410"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    multi_test_evasion()\n",
        "    multi_test_poison()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== testing GCN on original(clean) graph ===\n",
            "probs: [9.5474003e-03 6.7936694e-03 9.1732377e-03 6.4560480e-02 5.0589041e-04\n",
            " 8.9606100e-01 1.3358367e-02]\n",
            "Test set results: accuracy= 0.8300\n",
            "=== testing GCN on perturbed graph ===\n",
            "probs: [1.08786386e-04 1.94418699e-05 9.07947979e-05 7.05818311e-05\n",
            " 1.56384442e-06 1.00363675e-03 9.98705149e-01]\n",
            "Test set results: accuracy= 0.8325\n",
            "=== [Evasion] Attacking 40 nodes respectively ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:24<00:00,  1.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "misclassification rate : 0.975\n",
            "=== [Poisoning] Attacking 40 nodes respectively ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:34<00:00,  1.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "misclassification rate : 0.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwaTBn5OTar-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}